import requests
from bs4 import BeautifulSoup
import os
import time
import json
import re
from datetime import datetime
from urllib.parse import urlparse

class HiveBlogDownloader:
    """Hiveåšå®¢ä¸‹è½½å™¨ - åŠŸèƒ½å®Œæ•´ç‰ˆ"""
    
    def __init__(self, username, save_folder=None, max_retries=3, timeout=10, delay=1.5, debug=False):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            username: Hiveç”¨æˆ·å
            save_folder: ä¿å­˜æ–‡ä»¶å¤¹ï¼Œé»˜è®¤ä¸ºhive_blog_{username}
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            timeout: è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
            debug: è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        self.username = username
        self.base_url = f"https://hive.blog/@{username}/posts"
        self.save_folder = save_folder or f"hive_blog_{username}"
        self.max_retries = max_retries
        self.timeout = timeout
        self.delay = delay
        self.debug = debug
        self.downloaded_count = 0
        self.failed_count = 0
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # åˆ›å»ºä¿å­˜æ–‡ä»¶å¤¹
        os.makedirs(self.save_folder, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        self.log_file = os.path.join(self.save_folder, 'download_log.json')
        self.download_record = self._load_download_record()
    
    def _load_download_record(self):
        """åŠ è½½å·²ä¸‹è½½è®°å½•ï¼Œé¿å…é‡å¤ä¸‹è½½"""
        if os.path.exists(self.log_file):
            try:
                with open(self.log_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {'downloaded': [], 'failed': [], 'total_count': 0}
        return {'downloaded': [], 'failed': [], 'total_count': 0}
    
    def _save_download_record(self):
        """ä¿å­˜ä¸‹è½½è®°å½•"""
        try:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(self.download_record, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®°å½•å¤±è´¥: {e}")
    
    def _sanitize_filename(self, title):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        title = re.sub(r'[<>:"/\\|?*]', '', title)
        # é™åˆ¶é•¿åº¦
        title = title[:100]
        return title.strip()
    
    def _request_with_retry(self, url):
        """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚"""
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, headers=self.headers, timeout=self.timeout)
                response.raise_for_status()
                return response
            except requests.exceptions.Timeout:
                print(f"â±ï¸ è¯·æ±‚è¶…æ—¶ (ç¬¬ {attempt+1}/{self.max_retries} æ¬¡)")
            except requests.exceptions.ConnectionError:
                print(f"ğŸ”Œ è¿æ¥é”™è¯¯ (ç¬¬ {attempt+1}/{self.max_retries} æ¬¡)")
            except requests.exceptions.RequestException as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ (ç¬¬ {attempt+1}/{self.max_retries} æ¬¡): {e}")
            
            if attempt < self.max_retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        return None
    
    def _extract_article_metadata(self, article_elem):
        """ä»æ–‡ç« å…ƒç´ ä¸­æå–å…ƒæ•°æ®"""
        try:
            title_elem = article_elem.find('a')
            if not title_elem:
                return None
            
            title = title_elem.text.strip()
            href = title_elem.get('href', '')
            
            if not href:
                return None
            
            return {
                'title': title,
                'url': f"https://hive.blog{href}",
                'href': href
            }
        except Exception as e:
            print(f"âŒ æå–å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_article_content(self, article_soup, title, article_url):
        """ä»æ–‡ç« é¡µé¢ä¸­æå–å†…å®¹å’Œå…ƒä¿¡æ¯"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„å†…å®¹é€‰æ‹©å™¨
            content_elem = None
            content_selectors = [
                ('div', {'class': 'markdown-rendered'}),
                ('div', {'class': 'entry-content'}),
                ('article', {}),
                ('div', {'class': 'post-content'}),
                ('div', {'class': 'article-content'}),
                ('main', {}),
            ]
            
            for tag, attrs in content_selectors:
                if attrs:
                    content_elem = article_soup.find(tag, attrs)
                else:
                    content_elem = article_soup.find(tag)
                
                if content_elem:
                    if self.debug:
                        print(f"  â„¹ï¸ æ‰¾åˆ°å†…å®¹å…ƒç´ : {tag} {attrs}")
                    break
            
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–æ•´ä¸ªé¡µé¢çš„æ–‡æœ¬
            if not content_elem:
                if self.debug:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†å†…å®¹å…ƒç´ ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
                
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in article_soup(['script', 'style', 'nav', 'header', 'footer']):
                    script.decompose()
                
                # å°è¯•ä»bodyä¸­æå–
                body = article_soup.find('body')
                if body:
                    content_elem = body
                else:
                    if self.debug:
                        print(f"  âŒ æ— æ³•æ‰¾åˆ°ä»»ä½•å†…å®¹")
                    return None
            
            # æå–æ–‡æœ¬å†…å®¹
            if content_elem:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in content_elem(['script', 'style', 'nav', 'header', 'footer']):
                    script.decompose()
                
                # ä½¿ç”¨æ”¹è¿›çš„æ–‡æœ¬æå–æ–¹æ³•
                content = self._extract_structured_text(content_elem)
                
                if not content or len(content) < 50:  # å†…å®¹å¤ªçŸ­å¯èƒ½æ˜¯é”™è¯¯
                    if self.debug:
                        print(f"  âš ï¸ å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                    return None
                
                if self.debug:
                    print(f"  âœ“ æˆåŠŸæå–å†…å®¹ ({len(content)} å­—ç¬¦)")
            else:
                return None
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_elem = article_soup.find('span', class_='published')
            if not date_elem:
                date_elem = article_soup.find('time')
            publish_date = date_elem.text.strip() if date_elem else 'æœªçŸ¥'
            
            # æå–ä½œè€…
            author_elem = article_soup.find('a', class_='author-name')
            if not author_elem:
                author_elem = article_soup.find('span', class_='author')
            author = author_elem.text.strip() if author_elem else self.username
            
            return {
                'title': title,
                'content': content,
                'url': article_url,
                'author': author,
                'publish_date': publish_date,
                'download_date': datetime.now().isoformat()
            }
        except Exception as e:
            if self.debug:
                print(f"  âš ï¸ æå–å¼‚å¸¸: {str(e)[:100]}")
            return None
    
    def _extract_structured_text(self, elem):
        """æå–ä¿ç•™ç»“æ„çš„æ–‡æœ¬å†…å®¹"""
        content = elem.get_text(separator='\n').strip()
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        lines = []
        prev_empty = False
        for line in content.split('\n'):
            line = line.strip()
            if line:
                lines.append(line)
                prev_empty = False
            elif not prev_empty and lines:  # ä¿ç•™å•ä¸ªç©ºè¡Œåˆ†éš”
                lines.append('')
                prev_empty = True
        
        content = '\n'.join(lines)
        
        # é™åˆ¶é•¿åº¦
        if len(content) > 15000:
            content = content[:15000] + '\n\n[å†…å®¹å·²æˆªæ–­...]'
        
        return content
    
    def diagnose_page(self, url):
        """è¯Šæ–­é¡µé¢ç»“æ„ï¼Œå¸®åŠ©è°ƒè¯•"""
        print(f"\nğŸ” è¯Šæ–­é¡µé¢: {url}")
        try:
            response = self._request_with_retry(url)
            if not response:
                print("âŒ æ— æ³•è·å–é¡µé¢")
                return
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            print(f"âœ“ é¡µé¢å¤§å°: {len(response.text)} å­—èŠ‚")
            print(f"âœ“ é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å†…å®¹å®¹å™¨
            print("\nğŸ“‹ å¯èƒ½çš„å†…å®¹å®¹å™¨:")
            containers = [
                ('div.markdown-rendered', soup.find('div', class_='markdown-rendered')),
                ('div.entry-content', soup.find('div', class_='entry-content')),
                ('article', soup.find('article')),
                ('div.post-content', soup.find('div', class_='post-content')),
                ('main', soup.find('main')),
            ]
            
            for selector, elem in containers:
                if elem:
                    text_len = len(elem.get_text())
                    print(f"  âœ“ {selector}: {text_len} å­—ç¬¦")
                else:
                    print(f"  âœ— {selector}: æœªæ‰¾åˆ°")
            
        except Exception as e:
            print(f"âŒ è¯Šæ–­å¤±è´¥: {e}")
    
    def _save_article(self, article_data):
        """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
        try:
            title_clean = self._sanitize_filename(article_data['title'])
            filename = os.path.join(self.save_folder, f"{title_clean}.md")
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
            if os.path.exists(filename):
                timestamp = int(time.time())
                base, ext = os.path.splitext(filename)
                filename = f"{base}_{timestamp}{ext}"
            
            # æ ¼å¼åŒ–å¹¶ä¿å­˜å†…å®¹
            formatted_content = self._format_article_content(article_data)
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(formatted_content)
            
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _format_article_content(self, article_data):
        """æ ¼å¼åŒ–æ–‡ç« å†…å®¹ä¸ºç¾è§‚çš„Markdown"""
        title = article_data['title']
        author = article_data['author']
        publish_date = article_data['publish_date']
        download_date = article_data['download_date']
        url = article_data['url']
        content = article_data['content']
        
        # æ„å»ºæ ¼å¼åŒ–çš„Markdownæ–‡æ¡£
        formatted = f"""# {title}

---

**åŸºæœ¬ä¿¡æ¯**

| å­—æ®µ | å†…å®¹ |
|------|------|
| ä½œè€… | {author} |
| å‘å¸ƒæ—¥æœŸ | {publish_date} |
| åŸæ–‡é“¾æ¥ | [{url}]({url}) |

---

## æ–‡ç« æ­£æ–‡

{content}

---

*æœ¬æ–‡æ¡£ç”±Hiveåšå®¢ä¸‹è½½å·¥å…·è‡ªåŠ¨ç”Ÿæˆ | ä¸‹è½½æ—¶é—´: {download_date}*
""".strip()
        
        return formatted
    
    def download(self, max_pages=None):
        """
        ä¸‹è½½åšæ–‡
        
        Args:
            max_pages: æœ€å¤šä¸‹è½½é¡µæ•°ï¼ŒNoneè¡¨ç¤ºä¸‹è½½æ‰€æœ‰
        """
        print(f"\nğŸ“š å¼€å§‹ä¸‹è½½ {self.username} çš„Hiveåšå®¢æ–‡ç« ...")
        print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶å¤¹: {os.path.abspath(self.save_folder)}")
        print("-" * 60)
        
        start_time = time.time()
        page = 1
        total_attempted = 0
        
        while max_pages is None or page <= max_pages:
            try:
                url = f"{self.base_url}?page={page}"
                print(f"ğŸ”„ æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
                
                response = self._request_with_retry(url)
                if not response:
                    print(f"âš ï¸ ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œåœæ­¢ä¸‹è½½")
                    break
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
                articles = soup.find_all('h2', class_='articles__h2 entry-title')
                
                if not articles:
                    print(f"â„¹ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ–‡ç« ï¼Œä¸‹è½½å®Œæˆ")
                    break
                
                print(f"ğŸ“„ æœ¬é¡µæ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                
                # éå†æ¯ç¯‡æ–‡ç« 
                for idx, article in enumerate(articles, 1):
                    article_meta = self._extract_article_metadata(article)
                    if not article_meta:
                        continue
                    
                    title = article_meta['title']
                    article_url = article_meta['url']
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                    if article_url in self.download_record['downloaded']:
                        print(f"â­ï¸ è·³è¿‡å·²ä¸‹è½½: {title}")
                        continue
                    
                    # è·å–æ–‡ç« è¯¦ç»†å†…å®¹
                    article_response = self._request_with_retry(article_url)
                    if not article_response:
                        self.failed_count += 1
                        self.download_record['failed'].append({'title': title, 'url': article_url})
                        print(f"âŒ è·å–å¤±è´¥: {title}")
                        continue
                    
                    article_soup = BeautifulSoup(article_response.text, 'html.parser')
                    article_data = self._extract_article_content(article_soup, title, article_url)
                    
                    if not article_data:
                        self.failed_count += 1
                        self.download_record['failed'].append({'title': title, 'url': article_url})
                        print(f"âŒ æå–å†…å®¹å¤±è´¥: {title}")
                        continue
                    
                    # ä¿å­˜æ–‡ç« 
                    if self._save_article(article_data):
                        self.downloaded_count += 1
                        self.download_record['downloaded'].append(article_url)
                        print(f"âœ“ å·²ä¸‹è½½ ({self.downloaded_count}): {title}")
                    else:
                        self.failed_count += 1
                        self.download_record['failed'].append({'title': title, 'url': article_url})
                    
                    total_attempted += 1
                    time.sleep(self.delay)
                
                page += 1
                time.sleep(self.delay)
            
            except KeyboardInterrupt:
                print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
                break
            except Exception as e:
                print(f"âŒ ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
                break
        
        # ä¿å­˜è®°å½•
        self.download_record['total_count'] = self.downloaded_count
        self._save_download_record()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        print("-" * 60)
        print(f"âœ¨ ä¸‹è½½å®Œæˆï¼")
        print(f"âœ“ æˆåŠŸä¸‹è½½: {self.downloaded_count} ç¯‡")
        print(f"âŒ å¤±è´¥: {self.failed_count} ç¯‡")
        print(f"â±ï¸ è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {os.path.abspath(self.save_folder)}")
        print()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ Hiveåšå®¢ä¸‹è½½å·¥å…· v2.1")
    print("=" * 60)
    
    username = input("\nè¯·è¾“å…¥Hiveåšä¸»çš„ç”¨æˆ·å (ä¾‹å¦‚: rivalhw): ").strip()
    
    if not username:
        print("âŒ ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
        return
    
    # è¯¢é—®æ˜¯å¦ä½¿ç”¨è°ƒè¯•æ¨¡å¼
    debug_mode = input("å¯ç”¨è°ƒè¯•æ¨¡å¼? (y/n, é»˜è®¤n): ").strip().lower() == 'y'
    
    # å¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œå…ˆè¯Šæ–­ä¸€ä¸ªé¡µé¢
    if debug_mode:
        downloader = HiveBlogDownloader(username, debug=True)
        print("\né¦–å…ˆè¯Šæ–­ä¸€ä¸ªç¤ºä¾‹é¡µé¢...")
        downloader.diagnose_page(f"https://hive.blog/@{username}/posts")
    
    # è¯¢é—®å…¶ä»–é€‰é¡¹
    max_pages_input = input("è¯·è¾“å…¥æœ€å¤šä¸‹è½½çš„é¡µæ•° (æŒ‰Enterä¸‹è½½æ‰€æœ‰): ").strip()
    max_pages = int(max_pages_input) if max_pages_input.isdigit() else None
    
    delay_input = input("è¯·è¾“å…¥è¯·æ±‚é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤1.5ï¼‰: ").strip()
    delay = float(delay_input) if delay_input else 1.5
    
    # åˆ›å»ºä¸‹è½½å™¨å¹¶å¼€å§‹ä¸‹è½½
    downloader = HiveBlogDownloader(username, delay=delay, debug=debug_mode)
    downloader.download(max_pages=max_pages)


if __name__ == "__main__":
    main()
